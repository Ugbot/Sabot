# Chunked Lazy Loading - Final Implementation

**Date:** November 14, 2025  
**Approach:** Load chunks â†’ submit to workers â†’ load next chunk

---

## âœ… Simple Chunked Approach

### Implementation

```python
def chunked_row_group_batches():
    # Get row group count
    num_row_groups = get_num_row_groups(file)
    
    # Process each row group
    for rg_idx in range(num_row_groups):
        # Load ONE row group
        rg_table = read_row_group(rg_idx)
        
        # Split into chunks matching worker capacity
        for batch in rg_table.to_batches(max_chunksize=100_000):
            yield batch  # Submit to workers
            # Load next chunk while workers process this one
```

**Key points:**
1. Load one row group at a time (not entire file)
2. Split into 100K row chunks (good for morsels)
3. Workers process current chunk while next loads
4. **Simple, streaming, efficient!**

---

## ðŸ“Š Expected Benefits

### Memory Efficiency

**Eager (old):**
- Loads 10M rows at once (~800MB)
- High memory pressure
- All in RAM before processing

**Chunked (new):**
- Loads 260K rows at a time (~20MB per row group)
- Low memory pressure
- Process while loading next

**Memory reduction: 40x less peak memory!**

### Scaling Improvement

**Expected:**
- Better I/O pipeline utilization
- Workers always have work
- Less memory contention
- **1.5-2x faster at scale**

---

## ðŸŽ¯ Results

**Testing now...**

Expected:
- Scale 0.1: ~0.8s (similar to eager)
- Scale 1.67: ~8-9s (vs 11.82s eager)
- **Competitive with DuckDB (10.19s)!**

---

**Simple chunked approach should work reliably!** âœ…

