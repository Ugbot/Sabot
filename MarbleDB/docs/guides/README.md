# MarbleDB

**A high-performance, distributed analytical database built on Apache Arrow**

MarbleDB combines QuestDB-style ingestion performance with ClickHouse-class analytical capabilities and ArcticDB-style bitemporal versioning, all distributed via Raft consensus for strong consistency.

**Perfect for**: Time-series analytics, real-time analytics, financial data systems, IoT platforms, and event streaming with analytical storage.

## ‚ú® Key Features

### üöÄ Practical Ingestion
- **Flight-based ingestion**: Arrow RecordBatch append via DoPut
- **Time partitioning**: Configurable windows (1h/day) for query optimization
- **Concurrent writers**: Basic multi-writer support with fairness
- **Reasonable latency**: Data visible within seconds

### üìä Analytical Performance
- **Index-based queries**: Zone maps and bloom filters for fast filtering
- **Vectorized operations**: SIMD acceleration for common query patterns
- **Aggregation support**: COUNT, SUM, AVG on numeric columns
- **Query optimization**: Basic filter pushdown and predicate evaluation

### ‚è∞ ArcticDB-Style Time Travel ‚úÖ IMPLEMENTED
- **Full bitemporal support**: System time (MVCC) + Valid time (business validity)
- **AS OF queries**: Point-in-time historical queries
- **VALID_TIME queries**: Business validity range queries
- **Temporal reconstruction**: Conflict resolution and version chaining
- **Snapshot isolation**: Transaction-level consistency
- **Version history**: Complete audit trails per record

### üåê Distributed Reliability
- **Raft consensus**: Basic cluster coordination with 3-5 nodes
- **Crash recovery**: WAL-based restart with minimal data loss
- **Cluster management**: Add/remove nodes for scaling
- **Fault tolerance**: Survive single node failures

### üîÑ Streaming Support
- **Real-time tailing**: Follow newly ingested data
- **Basic CDC**: Change data capture for downstream processing
- **Cursor management**: Position tracking for exactly-once semantics

### üèóÔ∏è Practical Architecture

**Phase 1**: Simple file-based storage
```
Table (logical unit)
‚îú‚îÄ‚îÄ Partition (time-based: 1h/day)
‚îÇ   ‚îî‚îÄ‚îÄ Feather File (Arrow columnar format)
‚îÇ       ‚îî‚îÄ‚îÄ Basic metadata (row count, schema)
```

**Phase 2**: Indexed analytical storage
```
Table
‚îú‚îÄ‚îÄ Partition (time-based)
‚îÇ   ‚îî‚îÄ‚îÄ Data File (Arrow + zone maps)
‚îÇ   ‚îî‚îÄ‚îÄ Index File (bloom filters, metadata)
‚îî‚îÄ‚îÄ Manifest (partition metadata + statistics)
```

**Phase 3+**: Distributed segment-based
```
Cluster
‚îú‚îÄ‚îÄ Node (Raft participant)
‚îÇ   ‚îî‚îÄ‚îÄ Segment (immutable, replicated)
‚îÇ       ‚îú‚îÄ‚îÄ Stripe (column groups)
‚îÇ       ‚îî‚îÄ‚îÄ Footer (statistics, bloom filters)
‚îî‚îÄ‚îÄ Raft Log (distributed consensus)
```

**Design Principles**:
- **Start simple**: File-based storage first, optimize later
- **Arrow native**: Zero-copy where possible, conversion where practical
- **Incremental complexity**: Add features as needed, not prematurely

## üöÄ Quick Start

### Installation

```bash
# Clone with submodules (includes Arrow, NuRaft, etc.)
git clone --recursive https://github.com/your-org/MarbleDB.git
cd MarbleDB

# Build everything (one command!)
mkdir build && cd build
cmake .. -DMARBLE_ENABLE_RAFT=ON -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)

# Run the full production demo
make marble_raft_full_example
./marble_raft_full_example
```

### Basic Usage

```cpp
#include "marble/raft.h"

// Create a distributed cluster
MarbleRaftCluster cluster("my-cluster", {"localhost:50051", "localhost:50052"});
cluster.Start();

// Ingest time-series data
cluster.ExecuteWalOperation(R"(
    INSERT INTO sensor_data (timestamp, sensor_id, temperature, humidity)
    VALUES (NOW(), 'sensor_001', 23.5, 65.2)
)");

// Query with time travel
cluster.ExecuteSchemaOperation(R"(
    SELECT * FROM sensor_data
    WHERE timestamp >= '2024-01-01'
      AND sensor_id = 'sensor_001'
    FOR SYSTEM_TIME AS OF <snapshot_id>
)");
```

## üßä ArcticDB-Style Bitemporal Capabilities

MarbleDB implements ArcticDB's powerful bitemporal versioning system:

```cpp
#include "marble/temporal_reconstruction.h"

// Create bitemporal table
auto temporal_db = CreateTemporalDatabase("./arctic_db");
auto table = temporal_db->GetTemporalTable("employees");

// Insert data with business validity periods
ArcticOperations::AppendWithValidity(table, employee_data,
                                   valid_from_timestamp, valid_to_timestamp);

// AS OF queries - system time travel
auto as_of_query = CreateArcticQueryBuilder()
    ->AsOf(snapshot_from_past)
    ->Execute(table, &result);

// Valid time queries - business validity
auto valid_query = CreateArcticQueryBuilder()
    ->ValidBetween(start_date, end_date)
    ->Execute(table, &result);

// Full bitemporal reconstruction
auto bitemporal_query = CreateArcticQueryBuilder()
    ->AsOf(system_snapshot)
    ->ValidBetween(business_start, business_end)
    ->Execute(table, &result);

// Version history per record
std::vector<VersionInfo> history;
ArcticOperations::GetVersionHistory(table, "employee_id_123", &history);
```

**Key ArcticDB Features:**
- **System Time**: When data was written (MVCC snapshots)
- **Valid Time**: When data was actually valid in business terms
- **Temporal Reconstruction**: Rebuild historical state at any point
- **Version Chaining**: Handle overlapping validity periods
- **Conflict Resolution**: Determine active versions automatically

## üìö API Overview

### Embedded C++/Python API

```cpp
// Database lifecycle
auto db = MarbleDB::Open("/path/to/db");
db->CreateTable(schema);
db->Append(record_batch);
auto snapshot = db->Commit();

// Query interface
ScanSpec spec;
spec.columns = {"timestamp", "value"};
spec.filter = "timestamp > '2024-01-01'";
spec.as_of = snapshot_id;
auto result = db->Scan(spec);
```

### Flight & ADBC Integration

```cpp
// Streaming ingestion
flight_client.DoPut(descriptor, record_batch);

// Query execution with pruning
auto result = flight_client.DoGet(scan_spec_descriptor);
```

## üß™ Examples & Demos

### Core Examples
- **`distributed_raft_example`**: Basic 3-node Raft cluster
- **`marble_raft_cluster_example`**: WAL integration with Raft
- **`marble_raft_full_example`**: Complete production cluster with config management

### Run Examples

```bash
cd build

# Basic distributed cluster
make distributed_raft_example && ./distributed_raft_example

# Full production demo
make marble_raft_full_example && ./marble_raft_full_example

# Run all tests
ctest --output-on-failure
```

## üèóÔ∏è Architecture Deep Dive

### Storage Hierarchy

MarbleDB's Arrow-first architecture provides:

1. **Zero-copy access**: Direct mmap of compressed Arrow IPC pages
2. **Multi-level pruning**: Manifest ‚Üí Zone Maps ‚Üí Bloom Filters ‚Üí Dictionary checks
3. **Time-series optimization**: Automatic partitioning + clustering for analytical workloads
4. **Bitemporal support**: System time + valid time with efficient overlay reconstruction

### Pruning Strategy

The multi-level pruning stack delivers ClickHouse-class performance:

- **Manifest pruning**: Partition/time window elimination (1000√ó+ reduction possible)
- **Zone maps**: Min/max/null counts + quantiles per page
- **Bloom filters**: Membership testing with <2% false positive rate
- **Dictionary checks**: Low-cardinality column optimizations
- **Adaptive replanning**: Adjust strategy based on observed selectivity

### Execution Engine

Vectorized kernels with SIMD optimization:

- **60-80% CPU utilization** in vectorized operations
- **Late materialization**: Filter keys first, fetch payloads second
- **NUMA awareness**: Thread placement for optimal memory access
- **Cache-aligned batches**: 64k row processing units

### Distributed Layer

Raft consensus provides strong consistency:

- **State machines**: WAL replication + schema coordination
- **Persistent logs**: Crash-recoverable operation history
- **Dynamic membership**: Add/remove nodes without downtime
- **Arrow Flight transport**: High-performance inter-node communication

## üìä Realistic Performance Targets

### Phase 1 (MVP) - Baseline Performance
- **Ingestion**: 1-5 MB/s per core (establish baseline)
- **Query**: Basic table scans with time filtering
- **Storage**: Reliable Arrow-based persistence

### Phase 2 (Analytical) - Performance Focus
- **Ingestion**: 5-20 MB/s per core with optimization
- **Query Speedup**: 5-20√ó improvement with zone maps/bloom filters
- **Dataset Size**: Handle 10-100GB datasets efficiently
- **Index Usage**: 50-80% of queries use indexes

### Phase 3 (Distributed) - Production Performance
- **Fault Tolerance**: Survive single node failures
- **Recovery Time**: <1 minute from crashes
- **Cluster Scaling**: Linear throughput with node addition
- **Monitoring**: Basic operational metrics

### Phase 4 (Advanced) - Competitive Performance
- **Query Speedup**: 100-1000√ó gains on selective queries
- **Time Travel**: Efficient bitemporal reconstruction
- **Streaming**: Sub-second end-to-end latency
- **Scale**: Cloud-native with auto-scaling

## üó∫Ô∏è Implementation Strategy (Realistic Execution Plan)

### Phase 1: Foundation (3-4 months) - Working MVP
**Goal**: End-to-end analytical database that works
- ‚úÖ Basic Arrow storage with Feather files
- ‚úÖ Flight ingestion (DoPut RecordBatches)
- ‚úÖ Basic time-partitioned scanning
- üîÑ Simple embedded API

**Milestone**: Store/query sensor data by time range

### Phase 2: Analytical Performance (3-4 months) - Make it Fast
**Goal**: ClickHouse-competitive query performance
- üîÑ Zone maps and bloom filters
- üîÑ SIMD vectorized scanning
- üîÑ Basic aggregations (COUNT/SUM/AVG)
- üîÑ Query optimization

**Milestone**: 5-20√ó speedup with indexing, handle 10-100GB datasets

### Phase 3: Production Ready (4-6 months) - Reliable & Scalable
**Goal**: Fault-tolerant distributed system
- ‚úÖ Raft consensus (3-5 node clusters)
- üîÑ WAL and crash recovery
- üîÑ Basic compaction
- üîÑ Cluster management

**Milestone**: Survive node failures, horizontal scaling

### Phase 4: Advanced Analytics (6-12 months) - Unique Value
**Goal**: Features that justify the architecture complexity
- üîÑ Bitemporal time travel
- üîÑ Multi-level pruning (100-1000√ó gains)
- üîÑ Streaming analytics
- üîÑ Cloud storage integration

**Milestone**: Complex analytical queries with time travel

---

## üåü MarbleDB Grand Vision (The Complete Technical Plan)

This outlines the **full scope** of what MarbleDB could become - the comprehensive vision that guides long-term development. This is **aspirational** and represents the complete feature set rather than immediate implementation priorities.

### 1. Ingestion: QuestDB-Class Performance
- **Flight DoPut Streaming**: Arrow RecordBatch ingestion with backpressure
- **Time-Based Partitioning**: Configurable windows (1m/5m/1h/day) with automatic routing
- **Multi-Column Clustering**: Sort keys like (symbol, tenant, shard) for query optimization
- **Concurrent Writers**: Lock-free staging with fair queuing
- **Schema Evolution**: Additive fields with automatic null backfill
- **Performance Target**: >50 MB/s/core sustained throughput

### 2. Storage: Arrow-First Architecture
```
Segment (immutable, atomic unit)
‚îú‚îÄ‚îÄ Stripe (co-accessed columns)
‚îÇ   ‚îú‚îÄ‚îÄ Column Chunk (contiguous values)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Page (Arrow IPC buffers, compressed)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Footer (checksums, statistics, bloom filters)
‚îî‚îÄ‚îÄ Segment Footer (manifest, row counts, min/max timestamps)
```
- **Zero-copy access**: Direct mmap of Arrow IPC pages
- **Multi-level pruning**: Manifest ‚Üí Zone Maps ‚Üí Bloom ‚Üí Dictionary checks
- **Raft-replicated manifests**: Consistent metadata across cluster

### 3. Bitemporal Time Travel: ArcticDB-Style
- **System Time**: MVCC snapshots with `AS OF <timestamp | snapshot_id>`
- **Valid Time**: `valid_from`, `valid_to` columns for temporal modeling
- **Delete Vectors**: Efficient overlay for point-in-time reconstruction
- **Interval Indexing**: Fast temporal range queries
- **Diff queries**: Compare data between snapshots

### 4. Query Engine: ClickHouse-Class Analytics
**Pruning Stack** (in evaluation order):
1. **Manifest pruning**: Partition/time window elimination
2. **Zone maps**: Min/max/nulls + quantiles per page
3. **Bloom filters**: Membership testing (fixed-width + token/n-gram)
4. **Dictionary checks**: Low-cardinality optimizations
5. **Adaptive replanning**: Adjust strategy based on selectivity

**Execution**: Vectorized kernels with 60-80% CPU utilization in SIMD operations.

### 5. Distributed Layer: Raft + Arrow Flight
- **State Machines**: WAL replication + schema coordination + manifest commits
- **Persistent Logs**: Crash-recoverable operation history
- **Dynamic Membership**: Add/remove nodes without downtime
- **Arrow Flight transport**: High-performance inter-node communication
- **Jepsen-tested**: Linearizability guarantees

### 6. Advanced Features: Market Differentiation
- **Streaming & CDC**: Tailing cursors, change data capture, exactly-once semantics
- **Enterprise Security**: mTLS, API tokens, row-level access control, audit logging
- **Cloud Integration**: S3/GCS support, auto-scaling, multi-region deployments
- **GPU Acceleration**: CUDA/ROCm for heavy analytical workloads
- **Deep Ecosystem**: ADBC drivers, BI connectors, language SDKs

### 7. Performance Vision
- **Ingestion**: 50+ MB/s/core sustained throughput
- **Queries**: 100-1000√ó data reduction via pruning
- **Time Travel**: Efficient bitemporal reconstruction
- **Streaming**: Sub-second end-to-end latency
- **Distributed**: Linear scaling across 100+ nodes

**Unique Value**: Combines QuestDB's ingestion + ClickHouse's analytics + ArcticDB's time travel + TiDB's consistency in a single Arrow-native platform.

## ü§ù Contributing

### Development Setup

```bash
# Fork and clone
git clone --recursive https://github.com/your-org/MarbleDB.git
cd MarbleDB

# Build with all features
mkdir build && cd build
cmake .. -DMARBLE_ENABLE_RAFT=ON -DCMAKE_BUILD_TYPE=Debug
make -j$(nproc)

# Run tests
ctest --output-on-failure

# Run specific example
make marble_raft_full_example && ./marble_raft_full_example
```

### Code Organization

```
MarbleDB/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ raft/           # Distributed consensus layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raft_server.cpp         # Main Raft implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ marble_wal_state_machine.cpp  # WAL replication
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ marble_log_store.cpp     # Persistent log storage
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ arrow_flight_transport.cpp    # Inter-node transport
‚îÇ   ‚îî‚îÄ‚îÄ core/            # Core database engine
‚îú‚îÄ‚îÄ include/marble/     # Public APIs
‚îú‚îÄ‚îÄ examples/           # Usage examples
‚îî‚îÄ‚îÄ docs/               # Documentation
```

### Key Areas for Contribution

1. **Execution Engine**: SIMD optimizations, new vectorized operators
2. **Pruning & Indexing**: Zone maps, bloom filters, advanced index types
3. **Storage Layer**: Arrow IPC optimizations, compression algorithms
4. **Distributed Systems**: Raft improvements, cluster management
5. **Ecosystem Integration**: Flight/ADBC drivers, language bindings

### Testing Philosophy

- **Property-based testing** for core algorithms
- **Chaos engineering** for distributed correctness
- **Performance regression testing** on every commit
- **Jepsen testing** for distributed consistency guarantees

## üìä Current Capabilities vs. Competition

**Phase 1 (MVP)**: Basic time-series database
- ‚úÖ Arrow-native storage and ingestion
- ‚úÖ Time-partitioned queries
- ‚úÖ Basic analytical operations
- ‚ö†Ô∏è Limited to single-node, no advanced indexing

**Phase 2 (Analytical)**: ClickHouse competitor
- ‚úÖ High-performance indexed queries
- ‚úÖ SIMD-accelerated operations
- ‚úÖ Zone maps and bloom filters
- ‚ö†Ô∏è Still single-node, simpler feature set

**Phase 3 (Distributed)**: Fault-tolerant analytical database
- ‚úÖ Raft-based strong consistency
- ‚úÖ Multi-node horizontal scaling
- ‚úÖ Crash recovery and fault tolerance
- ‚ö†Ô∏è Not yet at full ClickHouse performance levels

**Phase 4+ (Advanced)**: Unique analytical database
- ‚úÖ Bitemporal time travel
- ‚úÖ Streaming analytics
- ‚úÖ Multi-level pruning (100-1000√ó gains)
- üéØ **Differentiation**: Combines ingestion + analytics + time travel + strong consistency

**Realistic Positioning**: MarbleDB aims to be the **analytical database that "just works"** - reliable, fast, and feature-complete enough for 80% of use cases, without the operational complexity of distributed systems like ClickHouse.

## üìû Community & Support

- **GitHub Issues**: Bug reports and feature requests
- **Discussions**: Architecture decisions and RFCs
- **Discord/Slack**: Real-time community support
- **Documentation**: Comprehensive guides and API reference

## üìÑ License

Licensed under the Apache License 2.0. See [LICENSE](LICENSE) for details.

---

**MarbleDB**: The analytical database that brings time-series ingestion, analytical performance, and bitemporal time travel together with distributed consistency. Built on Apache Arrow for the modern data stack. üöÄ
