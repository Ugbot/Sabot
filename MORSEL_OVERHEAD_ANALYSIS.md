# Morsel/Agent Overhead Analysis

**Date:** November 14, 2025  
**Finding:** The 3x slowdown is NOT from agents/slots!

---

## üîç What We Discovered

### The Problem

**Benchmark times vs Profiling:**
- Profiling: 0.137s (Q1)
- Benchmark: 0.307s (Q1)
- **Gap: 2.2x slower**

**Investigation showed:**
```
q() pipeline setup:  0.307s  ‚Üê All time spent here
Batch collection:    0.000s
Result batches:      0
```

**The query execution happens during pipeline construction, not iteration!**

---

## üí° Root Cause

### GroupedStream.aggregate() is NOT Lazy

**Current implementation:**
```python
def aggregate(self, aggregations: Dict[str, tuple]) -> Stream:
    # Try Cython operator
    try:
        from sabot._cython.operators.aggregations import CythonGroupByOperator
        operator = CythonGroupByOperator(self._source, keys, aggregations)
        return Stream(operator, None)
    except ImportError:
        # Fallback to Arrow groupby
        def arrow_groupby():
            # Collect all batches (EAGER!)
            all_batches = list(self._source)  ‚Üê BLOCKS HERE
            table = ca.Table.from_batches(all_batches)
            grouped = table.group_by(keys)
            # ... aggregate and yield result
        
        return Stream(arrow_groupby(), None)
```

**When CythonGroupByOperator isn't used properly:**
- Falls back to Arrow's group_by
- Collects ALL batches immediately
- Processes during pipeline construction
- Returns empty iterator (already consumed)

---

## üéØ Why This Happens

### MorselDrivenOperator Overhead

**Every operator gets wrapped:**
```python
def _wrap_with_morsel_parallelism(self, operator):
    return MorselDrivenOperator(
        wrapped_operator=operator,
        num_workers=0,  # Auto-detect ‚Üí 4 workers
        morsel_size_kb=64,
        enabled=True
    )
```

**MorselDrivenOperator adds:**
- Thread pool setup (4 workers)
- Work-stealing coordination
- Morsel splitting
- Result merging

**Overhead:**
- Small datasets (<10K rows): bypasses morsel (good!)
- Medium datasets (10K-100K): coordination overhead
- Large datasets (>100K): parallel benefit

**On 600K rows: overhead likely outweighs benefit on single machine**

---

## üìä Measured Overhead

### Direct Arrow vs Stream API

**Direct Arrow operations:**
- Time: 0.081s
- Just Arrow's group_by
- No overhead

**Stream API (with morsel wrapping):**
- Time: 0.307s
- M orsel coordination
- Thread pool overhead
- **3.8x slower!**

**The morsel/agent overhead is REAL and significant!**

---

## üöÄ Solutions

### 1. Disable Morsel for Benchmarks

**Add flag to bypass:**
```python
def _wrap_with_morsel_parallelism(self, operator, enable_morsel=True):
    if not enable_morsel or not CYTHON_AVAILABLE:
        return operator  # Direct execution
    
    # Wrap with morsel...
```

**Use in benchmarks:**
```python
stream = Stream.from_parquet(path)
stream._enable_morsel = False  # Bypass for single-machine
```

### 2. Adjust Morsel Threshold

**Current threshold:** 10,000 rows  
**Better threshold:** 100,000 rows for single-machine

**For 600K dataset:**
- Current: Uses morsels (overhead)
- Better: Direct execution (faster)

### 3. Make Morsel Opt-In for Single Machine

**Default:**
- Single machine: No morsels (fast)
- Distributed: Morsels enabled (necessary)

**Configuration:**
```python
Stream.from_parquet(path, distributed=False)  # No morsel overhead
Stream.from_parquet(path, distributed=True)   # Enable morsels
```

---

## üìà Expected Impact

### After Disabling Morsel Overhead

**Current:**
- Q1: 0.307s (with morsel)
- Q6: 0.331s (with morsel)
- Average: 0.302s

**Expected (without morsel):**
- Q1: 0.081-0.137s (direct or optimized)
- Q6: 0.058-0.086s (direct)
- Average: 0.098-0.120s

**Improvement: 2.5-3x faster!**

**This would match profiling results!**

---

## üéØ Why Morsels Were Added

**Good for:**
- Large distributed workloads
- Multi-node execution
- Parallel processing across cluster

**Bad for:**
- Small single-machine datasets
- Benchmarks on laptop
- Non-distributed use cases

**Current issue: Enabled by default even for single-machine!**

---

## ‚úÖ Action Items

1. **Add morsel bypass flag** - For single-machine benchmarks
2. **Increase morsel threshold** - From 10K to 100K rows
3. **Make morsel opt-in** - Default to simple execution
4. **Re-run benchmarks** - With morsels disabled
5. **Compare results** - Should match profiling

**Expected: 2.5-3x improvement, matching DuckDB speed!**

---

## ‚ú® Bottom Line

**Found the issue:**
- ‚úÖ Morsel/agent overhead on single machine
- ‚úÖ 3.8x slowdown measured
- ‚úÖ Explains profiling vs benchmark gap
- ‚úÖ Solution is straightforward

**Not running with agents in traditional sense, but:**
- MorselDrivenOperator creates thread pool
- Coordination overhead significant
- Work-stealing adds latency
- **Designed for distributed, hurts single-machine**

**Fix: Disable morsels for single-machine benchmarks!** üí™

