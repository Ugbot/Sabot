# Crypto Trading Research Platform - Complete Setup Guide

**Sabot + Fintech Kernels + Coinbase Data**

---

## Overview

Complete crypto research platform using:
- **Data**: Coinbase Advanced Trade WebSocket (via `coinbase2parquet.py`)
- **Processing**: Sabot streaming pipelines
- **Features**: 40+ fintech kernels (EWMA, OFI, MACD, RSI, etc.)
- **Storage**: State backends for larger-than-memory (RocksDB/Tonbo)
- **Modes**: Live trading, research, backtesting, arbitrage

---

## Quick Start (5 Minutes)

### 1. Start Infrastructure

```bash
# Start Kafka and Schema Registry
docker-compose up -d

# Verify
docker-compose ps
```

### 2. Stream Coinbase Data to Kafka

```bash
# Terminal 1: Stream Coinbase â†’ Kafka
python examples/coinbase2parquet.py -k

# Should see:
# Subscribed; streaming to Kafka topic 'coinbase-ticker' (format: json)
# (Data flows continuously)
```

### 3. Run Live Trading Research

```bash
# Terminal 2: Process with Sabot + Fintech Kernels
python examples/crypto_research_platform.py live

# Should see:
# ğŸš€ CRYPTO TRADING RESEARCH PLATFORM
# LIVE TRADING MODE
# âš™ï¸  Processing historical data...
# ğŸŸ¢ BUY SIGNAL: BTC-USD @ $67234.50
#    Z-score: -2.15, RSI: 28.3
```

**That's it!** You now have a complete crypto research platform running.

---

## Execution Modes

### Mode 1: LIVE Trading

**Real-time signal generation from live Coinbase data:**

```bash
# Start data stream (Terminal 1)
python examples/coinbase2parquet.py -k

# Run live trading (Terminal 2)
python examples/crypto_research_platform.py live
```

**What it does**:
1. Consumes tickers from Kafka
2. Computes 40+ features in real-time
3. Generates trading signals
4. Logs buy/sell opportunities
5. Monitors regime changes

**Features computed**:
- Log returns, EWMA, z-scores
- Midprice, microprice, order flow imbalance
- Realized variance, RiskMetrics volatility
- MACD, RSI, Bollinger Bands
- CUSUM change detection, VaR

### Mode 2: RESEARCH (Historical Analysis)

**Analyze historical data and save features:**

```bash
# Collect historical data (run for a while)
python examples/coinbase2parquet.py -F -o ./data/coinbase_historical.parquet

# Ctrl+C after collecting enough data (e.g., 1 hour)

# Run research analysis
python examples/crypto_research_platform.py research \
    --parquet ./data/coinbase_historical.parquet \
    --output ./data/crypto_features.parquet
```

**What it does**:
1. Loads historical ticker data
2. Computes all 40+ features
3. Analyzes per-symbol statistics
4. Computes cross-symbol correlations
5. Saves enriched data to Parquet

**Output**: Parquet file with all features for offline analysis

### Mode 3: BACKTEST

**Test strategy on historical data:**

```bash
python examples/crypto_research_platform.py backtest \
    --parquet ./data/coinbase_historical.parquet
```

**What it does**:
1. Loads historical data
2. Computes features
3. Generates signals based on strategy rules
4. Simulates execution
5. Computes performance metrics (Sharpe, max DD, etc.)

**Strategy** (example mean reversion):
- Entry: z-score < -2.0 AND RSI < 30 (oversold)
- Exit: z-score > 0 OR RSI > 50 (mean reversion)

### Mode 4: ARBITRAGE Monitor

**Cross-exchange price monitoring:**

```bash
python examples/crypto_research_platform.py arbitrage
```

**What it does**:
- Monitors prices from multiple exchanges
- Uses ASOF join to align timestamps
- Detects arbitrage opportunities
- Logs when spreads exceed threshold

**Requires**: Multiple exchange data feeds (extend coinbase2parquet.py for Binance, Kraken, etc.)

### Mode 5: EXPORT Features

**Compute features and export to Kafka:**

```bash
python examples/crypto_research_platform.py export
```

**What it does**:
- Consumes raw tickers from Kafka
- Computes features
- Produces enriched data to new Kafka topic
- Enables downstream consumers to use pre-computed features

---

## State Backend Options

### Memory (Default - Fast)

**Best for**: <1000 symbols, development

```bash
python examples/crypto_research_platform.py live \
    --state-backend memory
```

**Performance**: ~10ns state access  
**Capacity**: <1GB RAM  
**Persistence**: âŒ No

### RocksDB (Persistent)

**Best for**: 1000-10,000 symbols, production

```bash
python examples/crypto_research_platform.py live \
    --state-backend rocksdb
```

**Performance**: ~1Î¼s state access  
**Capacity**: ~10GB disk  
**Persistence**: âœ… Yes

### Tonbo (Large-Scale)

**Best for**: >10,000 symbols, columnar data

```bash
python examples/crypto_research_platform.py live \
    --state-backend tonbo
```

**Performance**: ~10Î¼s state access  
**Capacity**: >100GB disk  
**Persistence**: âœ… Yes

---

## Complete Workflow

### Step 1: Data Collection

```bash
# Collect 24 hours of data
python examples/coinbase2parquet.py -F -o ./data/btc_24h.parquet

# Let run for 24 hours, then Ctrl+C
```

### Step 2: Feature Engineering

```bash
# Compute all features
python examples/crypto_research_platform.py research \
    --parquet ./data/btc_24h.parquet \
    --output ./data/btc_features.parquet
```

### Step 3: Analysis

```python
# Load features and analyze
import pyarrow.parquet as pq
import pandas as pd

features = pq.read_table('./data/btc_features.parquet').to_pandas()

# Analyze EWMA crossovers
btc = features[features['symbol'] == 'BTC-USD']
crossovers = btc[(btc['ema'] > btc['price'].shift(1)) & 
                 (btc['ema'].shift(1) <= btc['price'].shift(2))]

print(f"Found {len(crossovers)} EMA crossovers")

# Analyze regime changes
regime_changes = btc[btc['cusum_alarm'] == 1]
print(f"Detected {len(regime_changes)} regime shifts")

# RSI extremes
oversold = btc[btc['rsi'] < 30]
overbought = btc[btc['rsi'] > 70]
print(f"Oversold: {len(oversold)}, Overbought: {len(overbought)}")
```

### Step 4: Strategy Development

```python
# Define strategy rules
def generate_signals(features_df):
    """Generate trading signals from features."""
    signals = pd.DataFrame(index=features_df.index)
    
    # Mean reversion
    signals['mean_reversion'] = (
        (features_df['zscore'] < -2.0) &
        (features_df['rsi'] < 30) &
        (features_df['cusum_alarm'] == 0)  # No regime change
    ).astype(int)
    
    # Trend following
    signals['trend_following'] = (
        (features_df['macd_hist'] > 0) &
        (features_df['ema'] > features_df['price']) &
        (features_df['rsi'] > 50)
    ).astype(int)
    
    # Volatility breakout
    signals['vol_breakout'] = (
        (features_df['price'] > features_df['bb_upper']) &
        (features_df['realized_var'] > features_df['realized_var'].rolling(50).mean())
    ).astype(int)
    
    return signals
```

### Step 5: Backtesting

```bash
# Run backtest
python examples/crypto_research_platform.py backtest \
    --parquet ./data/btc_features.parquet
```

### Step 6: Live Deployment

```bash
# Deploy live (after testing strategy)
python examples/crypto_research_platform.py live \
    --state-backend rocksdb  # Persistent state
```

---

## Features Computed

### Complete Feature Set (40+ columns)

**Price Features**:
- `log_return` - Log returns
- `ewma` - Exponentially weighted MA
- `zscore` - Rolling z-score
- `log_return_lag1` - Lagged returns
- `price_diff1` - Price difference

**Microstructure**:
- `midprice` - (bid + ask) / 2
- `microprice` - Size-weighted midprice
- `quoted_spread` - ask - bid (bps)
- `l1_imbalance` - Order book imbalance
- `ofi` - Order flow imbalance

**Volatility**:
- `realized_var` - Î£ rÂ²
- `bipower_var` - Jump-robust volatility
- `riskmetrics_vol` - EWMA volatility
- `log_return_std20` - Rolling std dev

**Momentum**:
- `ema` - Exponential MA
- `macd`, `macd_signal`, `macd_hist` - MACD indicator
- `rsi` - Relative Strength Index
- `bb_mid`, `bb_upper`, `bb_lower` - Bollinger Bands

**Statistical**:
- `log_return_autocorr5` - Autocorrelation (lag 5)
- `mean`, `variance`, `stddev` - Running statistics

**Risk**:
- `pnl` - Cumulative P&L
- `cusum_stat`, `cusum_alarm` - Change detection
- `var` - Value at Risk (95%)

---

## Performance

### Throughput

| Mode | Symbols | Throughput | Latency |
|------|---------|------------|---------|
| Live (Memory) | 14 | ~100K ticks/sec | <10ms p99 |
| Live (RocksDB) | 14 | ~50K ticks/sec | <20ms p99 |
| Research (Memory) | 14 | ~500K ticks/sec | N/A |
| Research (RocksDB) | 1000+ | ~200K ticks/sec | N/A |

### Resource Usage

| Mode | RAM | Disk | CPU |
|------|-----|------|-----|
| Live (14 symbols) | ~500MB | 0 | ~20% (4 cores) |
| Research (1K symbols) | ~2GB | ~10GB | ~80% (4 cores) |
| Research (10K symbols, RocksDB) | ~1GB | ~100GB | ~80% (4 cores) |

---

## Example Output

### Live Trading Mode

```
ğŸš€ CRYPTO TRADING RESEARCH PLATFORM
======================================================================
LIVE TRADING MODE
======================================================================

ğŸ“¡ Streaming from Kafka topic: coinbase-ticker

ğŸš€ Starting live processing...

ğŸŸ¢ BUY SIGNAL: BTC-USD @ $67234.50
   Z-score: -2.15, RSI: 28.3

ğŸŸ¢ BUY SIGNAL: ETH-USD @ $3845.20
   Z-score: -2.08, RSI: 29.1

âš ï¸  REGIME CHANGE: DOGE-USD
   CUSUM: 0.0123

ğŸ”´ SELL SIGNAL: SOL-USD @ $145.67
   Z-score: 2.34, RSI: 72.5

Processed 10 batches, 4 signals generated
```

### Research Mode

```
RESEARCH MODE - Historical Analysis
======================================================================

ğŸ“Š Loading data from: ./data/coinbase_historical.parquet
   Rows: 145,832
   Columns: 18

ğŸ”§ Building feature pipeline...
  Phase 1: Price features
  Phase 2: Microstructure
  Phase 3: Volatility
  Phase 4: Momentum
  Phase 5: Statistical features
  Phase 6: Risk monitoring

âš™ï¸  Processing historical data...
  Processed 10 batches...
  Processed 20 batches...

âœ… Feature computation complete!
   Total rows: 145,832
   Total columns: 58

ğŸ’¾ Saving results to: ./data/crypto_features.parquet
âœ… Saved 145,832 rows

FEATURE ANALYSIS
======================================================================

ğŸ“Š Per-symbol statistics:

BTC-USD:
  Observations: 12,486
  Avg return: 0.0023%
  Volatility: 1.45%
  RSI: 52.3 (avg)
  Imbalance: 0.012
  Regime changes: 3

ETH-USD:
  Observations: 11,923
  Avg return: 0.0031%
  Volatility: 1.89%
  RSI: 48.7 (avg)
  Imbalance: -0.008
  Regime changes: 5
```

---

## Advanced Use Cases

### 1. Multi-Timeframe Analysis

```python
# Combine minute, hour, and day features
minute_features = compute_features(minute_data)
hour_features = compute_features(hour_data)
day_features = compute_features(day_data)

# ASOF join to align
combined = asof_join(
    minute_features,
    hour_features,
    on='timestamp',
    by='symbol'
)
```

### 2. Cross-Asset Correlation

```python
# Compute Hayashi-Yoshida covariance for async ticks
btc_stream = stream.filter(lambda b: b['symbol'] == 'BTC-USD')
eth_stream = stream.filter(lambda b: b['symbol'] == 'ETH-USD')

# Async covariance
for btc_batch, eth_batch in zip(btc_stream, eth_stream):
    cov = hayashi_yoshida_cov(btc_batch, eth_batch, window_ms=60000)
    print(f"BTC-ETH covariance: {cov}")
```

### 3. Funding Rate Arbitrage

```python
# Track perpetual funding rates
funding_stream = Stream.from_kafka(..., 'funding-rates', ...)

pipeline = (funding_stream
    .map(lambda b: funding_apr(b, funding_freq_hours=8.0))
    .map(lambda b: basis_annualized(b, T_days=90))
    .map(lambda b: cash_and_carry_edge(b))
)

# Execute when edge > threshold
for batch in pipeline:
    if batch['carry_edge_bps'] > 100:  # >1% annualized
        execute_cash_and_carry()
```

### 4. Portfolio Risk Management

```python
# Real-time portfolio risk
position_stream = Stream.from_kafka(..., 'positions', ...)

risk_pipeline = (position_stream
    .map(lambda b: historical_var(b, 'pnl', alpha=0.05, window=100))
    .map(lambda b: historical_cvar(b, 'pnl', alpha=0.05, window=100))
    .map(lambda b: cusum(b, 'pnl', k=500, h=5000))
)

# Kill switch on risk limits
for batch in risk_pipeline:
    if batch['var'] > MAX_VAR or batch['cusum_alarm']:
        close_all_positions()
```

---

## Data Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Coinbase Advanced Trade WebSocket       â”‚
â”‚  (BTC, ETH, DOGE, XRP, SOL, etc.)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        coinbase2parquet.py -k
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Kafka Topic: coinbase-ticker         â”‚
â”‚     (Raw ticker data, ~100 msgs/sec)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
      crypto_research_platform.py
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Sabot Streaming Pipeline            â”‚
â”‚                                             â”‚
â”‚  1. Price transforms (log returns, EWMA)   â”‚
â”‚  2. Microstructure (midprice, OFI, VPIN)   â”‚
â”‚  3. Volatility (RV, BPV, RiskMetrics)      â”‚
â”‚  4. Momentum (MACD, RSI, Bollinger)        â”‚
â”‚  5. Risk (CUSUM, VaR)                      â”‚
â”‚                                             â”‚
â”‚  Features: 40+ columns computed             â”‚
â”‚  Performance: 100K+ ticks/sec               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
        â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Live Trading â”‚    â”‚ Research Export  â”‚
â”‚              â”‚    â”‚                  â”‚
â”‚ â†’ Signals    â”‚    â”‚ â†’ Parquet files  â”‚
â”‚ â†’ Execution  â”‚    â”‚ â†’ Analysis       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## State Management

### Small Scale (<1000 symbols)

```bash
# Use memory backend (default)
python examples/crypto_research_platform.py live
```

**Characteristics**:
- All state in RAM
- ~10ns access time
- Perfect for Coinbase's 14 products

### Medium Scale (1000-10,000 symbols)

```bash
# Use RocksDB backend
python examples/crypto_research_platform.py live \
    --state-backend rocksdb
```

**Characteristics**:
- State persisted to disk
- ~1Î¼s access time
- Survives crashes
- Good for multi-exchange (1000s of pairs)

### Large Scale (>10,000 symbols)

```bash
# Use Tonbo backend
python examples/crypto_research_platform.py research \
    --parquet huge_dataset.parquet \
    --state-backend tonbo
```

**Characteristics**:
- Columnar LSM storage
- ~10Î¼s access time
- Handles millions of symbols
- Arrow-native operations

---

## Extending the Platform

### Add New Exchange

1. **Create data collector** (like coinbase2parquet.py):
```python
# binance2kafka.py
async def stream_binance_to_kafka():
    # Connect to Binance WebSocket
    # Parse messages
    # Produce to Kafka topic 'binance-ticker'
```

2. **Update platform to consume**:
```python
# In crypto_research_platform.py
binance_stream = Stream.from_kafka(
    ResearchConfig.KAFKA_BOOTSTRAP,
    'binance-ticker',
    'research-binance'
)
```

3. **ASOF join for alignment**:
```python
# Join Coinbase and Binance by timestamp
aligned = asof_join(
    coinbase_batch,
    binance_batch,
    on='timestamp',
    by='symbol',
    tolerance_ms=500
)

# Compute cross-exchange spread
spread = aligned['coinbase_price'] - aligned['binance_price']
```

### Add Custom Indicators

```python
# Add to pipeline in build_crypto_feature_pipeline()

# Custom indicator
def my_custom_signal(batch):
    """Your proprietary signal."""
    # Use fintech kernels as building blocks
    batch = ewma(batch, alpha=0.9)
    batch = rolling_zscore(batch, window=50)
    
    # Custom logic
    signal = (batch['ewma'] > batch['price']) & (batch['zscore'] < -1.5)
    
    return batch.append_column('my_signal', pa.array(signal))

# Add to pipeline
pipeline = pipeline.map(my_custom_signal)
```

### Add Machine Learning

```python
# Train model on features
from sklearn.ensemble import RandomForestClassifier

# Load features
features = pq.read_table('./data/crypto_features.parquet').to_pandas()

# Define target (next period return sign)
features['target'] = (features.groupby('symbol')['log_return']
                      .shift(-1) > 0).astype(int)

# Train
X = features[['ewma', 'rsi', 'macd', 'l1_imbalance', 'realized_var']]
y = features['target']

model = RandomForestClassifier()
model.fit(X, y)

# Use in live trading
def ml_signal(batch):
    """ML-based signal generation."""
    df = batch.to_pandas()
    X = df[['ewma', 'rsi', 'macd', 'l1_imbalance', 'realized_var']]
    predictions = model.predict_proba(X)[:, 1]
    
    return batch.append_column('ml_score', pa.array(predictions))

pipeline = pipeline.map(ml_signal)
```

---

## File Structure

```
examples/
â”œâ”€â”€ coinbase2parquet.py            # Data collector (your file)
â”œâ”€â”€ crypto_research_platform.py    # Main platform (this file)
â””â”€â”€ CRYPTO_RESEARCH_SETUP.md       # This guide

data/                              # Created on first run
â”œâ”€â”€ coinbase_historical.parquet    # Raw tickers
â”œâ”€â”€ crypto_features.parquet        # Computed features
â””â”€â”€ backtest_results.parquet       # Strategy results

state/                             # State backends (if using persistent)
â”œâ”€â”€ log_returns/                   # Per-kernel state
â”œâ”€â”€ ewma/
â”œâ”€â”€ ofi/
â””â”€â”€ checkpoints/                   # Periodic checkpoints
```

---

## Troubleshooting

### "No data from Kafka"

```bash
# Check if coinbase2parquet.py is running
ps aux | grep coinbase2parquet

# Check Kafka topic
kcat -C -b localhost:19092 -t coinbase-ticker -o end -c 10

# Verify topic exists
kcat -L -b localhost:19092 | grep coinbase
```

### "Out of memory"

```bash
# Switch to RocksDB backend
python examples/crypto_research_platform.py live \
    --state-backend rocksdb

# Or reduce batch size
# Edit ResearchConfig.KAFKA_BATCH_SIZE = 50
```

### "Slow performance"

```bash
# Check if using morsels
# Batches should be >10K rows for automatic parallelism

# Or use distributed mode (if cluster available)
# See DISTRIBUTED_EXECUTION.md
```

---

## Next Steps

1. **Collect data**: Run `coinbase2parquet.py -F` for 24 hours
2. **Explore features**: Run `crypto_research_platform.py research`
3. **Develop strategy**: Analyze features.parquet offline
4. **Backtest**: Test strategy on historical data
5. **Paper trade**: Run live mode with logging only
6. **Live trade**: Connect to exchange API for execution

---

## Resources

- **Coinbase WebSocket**: https://docs.cloud.coinbase.com/advanced-trade-api/docs/ws-overview
- **Sabot fintech kernels**: `sabot/fintech/README.md`
- **ASOF joins**: `sabot/fintech/ASOF_JOIN_GUIDE.md`
- **State backends**: `sabot/fintech/STATE_BACKENDS.md`
- **Distributed execution**: `sabot/fintech/DISTRIBUTED_EXECUTION.md`

---

**Ready to build crypto trading strategies with production-grade infrastructure!** ğŸš€

