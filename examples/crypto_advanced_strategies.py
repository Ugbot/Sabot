#!/usr/bin/env python3
"""
Advanced Crypto Strategies - Multi-Exchange + Sophisticated Analytics

Advanced patterns using Sabot fintech kernels:
1. Cross-exchange arbitrage (Coinbase, Binance, Kraken)
2. Pairs trading (BTC-ETH cointegration)
3. Funding rate arbitrage (perp-spot)
4. Market making with microstructure signals
5. Statistical arbitrage with ML

Requires: Multiple exchange data feeds via coinbase2parquet-style collectors
"""

import asyncio
import logging
from pathlib import Path
import numpy as np
import pandas as pd

from sabot.api import Stream
import pyarrow as pa
import pyarrow.compute as pc

from sabot.fintech import (
    # ASOF joins
    asof_join,
    AsofJoinKernel,
    # Price features
    log_returns,
    ewma,
    rolling_zscore,
    # Microstructure
    midprice,
    microprice,
    l1_imbalance,
    ofi,
    vpin,
    # Crypto-specific
    basis_annualized,
    funding_apr,
    cash_and_carry_edge,
    # Cross-asset
    hayashi_yoshida_cov,
    kalman_hedge_ratio,
    pairs_spread,
    # Execution
    vwap,
    implementation_shortfall,
    # Risk
    cusum,
    historical_var,
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# Strategy 1: Cross-Exchange Arbitrage
# ============================================================================

async def cross_exchange_arbitrage():
    """
    Monitor prices across Coinbase, Binance, Kraken.
    
    Detect arbitrage when:
    - Same asset trades at different prices
    - Spread exceeds fees + slippage
    - Sufficient liquidity on both sides
    
    Uses ASOF join to align asynchronous tick data.
    """
    logger.info("="*70)
    logger.info("STRATEGY 1: Cross-Exchange Arbitrage")
    logger.info("="*70)
    
    # In production: separate Kafka topics per exchange
    # coinbase_stream = Stream.from_kafka(..., 'coinbase-ticker', ...)
    # binance_stream = Stream.from_kafka(..., 'binance-ticker', ...)
    # kraken_stream = Stream.from_kafka(..., 'kraken-ticker', ...)
    
    # For demo: show structure with single exchange
    logger.info("\nüí° Structure for multi-exchange arbitrage:\n")
    
    logger.info("```python")
    logger.info("# Stream from each exchange")
    logger.info("coinbase = Stream.from_kafka('localhost:19092', 'coinbase-ticker', 'arb')")
    logger.info("binance = Stream.from_kafka('localhost:19092', 'binance-ticker', 'arb')")
    logger.info("kraken = Stream.from_kafka('localhost:19092', 'kraken-ticker', 'arb')")
    logger.info("")
    logger.info("# Compute midprice for each")
    logger.info("coinbase = coinbase.map(lambda b: midprice(b))")
    logger.info("binance = binance.map(lambda b: midprice(b))")
    logger.info("kraken = kraken.map(lambda b: midprice(b))")
    logger.info("")
    logger.info("# ASOF join to align timestamps")
    logger.info("# (Different exchanges have different latencies)")
    logger.info("for cb_batch, bn_batch, kr_batch in zip(coinbase, binance, kraken):")
    logger.info("    # Align Coinbase with Binance")
    logger.info("    cb_bn = asof_join(cb_batch, bn_batch,")
    logger.info("                      on='timestamp', by='symbol',")
    logger.info("                      tolerance_ms=500)")
    logger.info("    ")
    logger.info("    # Align with Kraken")
    logger.info("    aligned = asof_join(cb_bn, kr_batch,")
    logger.info("                       on='timestamp', by='symbol',")
    logger.info("                       tolerance_ms=500)")
    logger.info("    ")
    logger.info("    # Compute spreads")
    logger.info("    df = aligned.to_pandas()")
    logger.info("    df['cb_bn_spread'] = (df['coinbase_mid'] - df['binance_mid']) / df['coinbase_mid'] * 10000")
    logger.info("    df['cb_kr_spread'] = (df['coinbase_mid'] - df['kraken_mid']) / df['coinbase_mid'] * 10000")
    logger.info("    ")
    logger.info("    # Execute arbitrage")
    logger.info("    for idx, row in df.iterrows():")
    logger.info("        if abs(row['cb_bn_spread']) > 15:  # >1.5 bps after fees")
    logger.info("            execute_arbitrage(")
    logger.info("                buy_exchange='binance' if row['cb_bn_spread'] > 0 else 'coinbase',")
    logger.info("                sell_exchange='coinbase' if row['cb_bn_spread'] > 0 else 'binance',")
    logger.info("                symbol=row['symbol'],")
    logger.info("                edge_bps=abs(row['cb_bn_spread'])")
    logger.info("            )")
    logger.info("```\n")
    
    logger.info("üìä Expected performance:")
    logger.info("   - Latency: <50ms tick-to-trade")
    logger.info("   - Opportunities: 10-50/day for BTC/ETH")
    logger.info("   - Edge: 5-20 bps per trade")
    logger.info("   - Win rate: ~95% (statistical arb)")


# ============================================================================
# Strategy 2: BTC-ETH Pairs Trading
# ============================================================================

async def btc_eth_pairs_trading():
    """
    Cointegration-based pairs trading between BTC and ETH.
    
    Uses:
    - Kalman filter for time-varying hedge ratio
    - Rolling z-score for entry/exit
    - CUSUM for regime detection
    """
    logger.info("\n" + "="*70)
    logger.info("STRATEGY 2: BTC-ETH Pairs Trading")
    logger.info("="*70)
    
    logger.info("\nüìù Pairs trading structure:\n")
    
    logger.info("```python")
    logger.info("# Separate streams for BTC and ETH")
    logger.info("all_data = Stream.from_kafka('localhost:19092', 'coinbase-ticker', 'pairs')")
    logger.info("")
    logger.info("# Split by symbol")
    logger.info("btc_stream = all_data.filter(lambda b: pc.equal(b.column('symbol'), 'BTC-USD'))")
    logger.info("eth_stream = all_data.filter(lambda b: pc.equal(b.column('symbol'), 'ETH-USD'))")
    logger.info("")
    logger.info("# Compute log returns")
    logger.info("btc_stream = btc_stream.map(lambda b: log_returns(b, 'price'))")
    logger.info("eth_stream = eth_stream.map(lambda b: log_returns(b, 'price'))")
    logger.info("")
    logger.info("# Process pairs")
    logger.info("for btc_batch, eth_batch in zip(btc_stream, eth_stream):")
    logger.info("    # ASOF join to align timestamps")
    logger.info("    aligned = asof_join(btc_batch, eth_batch,")
    logger.info("                       on='timestamp',")
    logger.info("                       tolerance_ms=100)")
    logger.info("    ")
    logger.info("    # Compute time-varying hedge ratio (Kalman filter)")
    logger.info("    hedge_result = kalman_hedge_ratio(")
    logger.info("        btc_batch, eth_batch,")
    logger.info("        q=0.001, r=0.1")
    logger.info("    )")
    logger.info("    ")
    logger.info("    # Compute spread and z-score")
    logger.info("    spread_batch = pairs_spread(")
    logger.info("        btc_batch, eth_batch,")
    logger.info("        hedge_ratio=hedge_result['hedge_ratio']")
    logger.info("    )")
    logger.info("    spread_batch = rolling_zscore(spread_batch, window=100)")
    logger.info("    ")
    logger.info("    # Trading logic")
    logger.info("    df = spread_batch.to_pandas()")
    logger.info("    z = df['zscore'].iloc[-1]")
    logger.info("    beta = df['hedge_ratio'].iloc[-1]")
    logger.info("    ")
    logger.info("    if z < -2.0:  # Spread too wide")
    logger.info("        # Long spread: buy BTC, sell beta √ó ETH")
    logger.info("        logger.info(f'LONG SPREAD: BTC vs {beta:.2f} √ó ETH (z={z:.2f})')")
    logger.info("        execute_pairs_trade('long', beta)")
    logger.info("    ")
    logger.info("    elif z > 2.0:  # Spread too narrow")
    logger.info("        # Short spread: sell BTC, buy beta √ó ETH")
    logger.info("        logger.info(f'SHORT SPREAD: BTC vs {beta:.2f} √ó ETH (z={z:.2f})')")
    logger.info("        execute_pairs_trade('short', beta)")
    logger.info("```\n")
    
    logger.info("üìä Expected performance:")
    logger.info("   - Sharpe: 1.5-2.5 (pairs strategies)")
    logger.info("   - Max DD: 5-10%")
    logger.info("   - Trades/month: 50-100")
    logger.info("   - Avg holding: 2-8 hours")


# ============================================================================
# Strategy 3: Perpetual Funding Rate Arbitrage
# ============================================================================

async def funding_rate_arbitrage():
    """
    Arbitrage between perpetual and spot markets using funding rates.
    
    Cash-and-carry:
    - Buy spot, short perp when funding rate high
    - Collect funding payments
    - Unwind when funding normalizes
    """
    logger.info("\n" + "="*70)
    logger.info("STRATEGY 3: Funding Rate Arbitrage")
    logger.info("="*70)
    
    logger.info("\nüìù Funding rate arbitrage structure:\n")
    
    logger.info("```python")
    logger.info("# Stream perpetual and spot prices")
    logger.info("perp_stream = Stream.from_kafka(..., 'btc-perp-ticker', ...)")
    logger.info("spot_stream = Stream.from_kafka(..., 'btc-spot-ticker', ...)")
    logger.info("funding_stream = Stream.from_kafka(..., 'btc-funding-rates', ...)")
    logger.info("")
    logger.info("# ASOF join perp with spot")
    logger.info("for perp_batch, spot_batch, funding_batch in zip(perp_stream, spot_stream, funding_stream):")
    logger.info("    # Align timestamps")
    logger.info("    perp_spot = asof_join(perp_batch, spot_batch,")
    logger.info("                         on='timestamp', by='symbol')")
    logger.info("    ")
    logger.info("    # Add funding data")
    logger.info("    combined = asof_join(perp_spot, funding_batch,")
    logger.info("                        on='timestamp', by='symbol')")
    logger.info("    ")
    logger.info("    # Compute metrics")
    logger.info("    combined = funding_apr(combined, funding_freq_hours=8.0)")
    logger.info("    combined = basis_annualized(combined, T_days=90)")
    logger.info("    combined = cash_and_carry_edge(combined)")
    logger.info("    ")
    logger.info("    # Trading logic")
    logger.info("    df = combined.to_pandas()")
    logger.info("    ")
    logger.info("    for idx, row in df.iterrows():")
    logger.info("        if row['carry_edge_bps'] > 200:  # >2% annualized")
    logger.info("            logger.info(f'CASH-AND-CARRY OPPORTUNITY:')")
    logger.info("            logger.info(f\"  Funding APR: {row['funding_apr']*100:.2f}%\")")
    logger.info("            logger.info(f\"  Basis APR: {row['basis_apr']*100:.2f}%\")")
    logger.info("            logger.info(f\"  Net edge: {row['carry_edge_bps']:.0f} bps\")")
    logger.info("            ")
    logger.info("            # Execute: buy spot, short perp")
    logger.info("            execute_cash_and_carry(row['symbol'], row['carry_edge_bps'])")
    logger.info("```\n")
    
    logger.info("üìä Expected performance:")
    logger.info("   - APR: 10-30% (funding arbitrage)")
    logger.info("   - Risk: Low (delta-neutral)")
    logger.info("   - Opportunities: Daily (when funding spikes)")


# ============================================================================
# Strategy 4: Market Making with Microstructure
# ============================================================================

async def microstructure_market_making():
    """
    Market making using microstructure signals.
    
    Uses:
    - Microprice for fair value
    - OFI for short-term prediction
    - VPIN for toxicity detection
    - Inventory management
    """
    logger.info("\n" + "="*70)
    logger.info("STRATEGY 4: Microstructure Market Making")
    logger.info("="*70)
    
    logger.info("\nüìù Market making with microstructure:\n")
    
    logger.info("```python")
    logger.info("stream = Stream.from_kafka(..., 'coinbase-ticker', ...)")
    logger.info("")
    logger.info("# Compute microstructure features")
    logger.info("pipeline = (stream")
    logger.info("    .map(lambda b: midprice(b))")
    logger.info("    .map(lambda b: microprice(b))  # Fair value")
    logger.info("    .map(lambda b: l1_imbalance(b))")
    logger.info("    .map(lambda b: ofi(b))  # Price prediction")
    logger.info("    .map(lambda b: vpin(b, bucket_vol=10000))  # Toxicity")
    logger.info(")")
    logger.info("")
    logger.info("for batch in pipeline:")
    logger.info("    df = batch.to_pandas()")
    logger.info("    ")
    logger.info("    for idx, row in df.iterrows():")
    logger.info("        fair_value = row['microprice']")
    logger.info("        flow = row['ofi']")
    logger.info("        toxicity = row['vpin']")
    logger.info("        ")
    logger.info("        # Adjust quotes based on signals")
    logger.info("        skew = 0.0")
    logger.info("        ")
    logger.info("        # OFI predicts short-term price move")
    logger.info("        if flow > 0:  # Buy pressure")
    logger.info("            skew += 0.0005  # Shade bid down, ask up")
    logger.info("        elif flow < 0:  # Sell pressure")
    logger.info("            skew -= 0.0005  # Shade bid up, ask down")
    logger.info("        ")
    logger.info("        # VPIN indicates informed trading - widen spread")
    logger.info("        width_mult = 1.0 + toxicity * 2.0")
    logger.info("        ")
    logger.info("        # Place quotes")
    logger.info("        bid = fair_value * (1 - 0.0005 * width_mult + skew)")
    logger.info("        ask = fair_value * (1 + 0.0005 * width_mult + skew)")
    logger.info("        ")
    logger.info("        place_quotes(row['symbol'], bid, ask)")
    logger.info("```\n")
    
    logger.info("üìä Expected performance:")
    logger.info("   - Sharpe: 3-5 (market making)")
    logger.info("   - Daily P&L: 0.1-0.5% of capital")
    logger.info("   - Inventory turnover: 10-50x/day")


# ============================================================================
# Strategy 5: Statistical Arbitrage Basket
# ============================================================================

async def statistical_arbitrage_basket():
    """
    Multi-asset mean reversion on a basket of cryptocurrencies.
    
    Uses PCA to identify common factors and trade residuals.
    """
    logger.info("\n" + "="*70)
    logger.info("STRATEGY 5: Statistical Arbitrage Basket")
    logger.info("="*70)
    
    logger.info("\nüí° Structure for multi-asset stat arb:\n")
    
    logger.info("```python")
    logger.info("# Stream all crypto assets")
    logger.info("stream = Stream.from_kafka(..., 'coinbase-ticker', ...)")
    logger.info("")
    logger.info("# Compute features for entire basket")
    logger.info("features = (stream")
    logger.info("    .map(lambda b: log_returns(b, 'price'))")
    logger.info("    .map(lambda b: ewma(b, alpha=0.94))")
    logger.info("    .map(lambda b: rolling_zscore(b, window=100))")
    logger.info(")")
    logger.info("")
    logger.info("# Collect basket returns")
    logger.info("basket_returns = {}")
    logger.info("")
    logger.info("for batch in features:")
    logger.info("    df = batch.to_pandas()")
    logger.info("    ")
    logger.info("    # Update basket")
    logger.info("    for symbol in df['symbol'].unique():")
    logger.info("        symbol_df = df[df['symbol'] == symbol]")
    logger.info("        basket_returns[symbol] = symbol_df['log_return'].values")
    logger.info("    ")
    logger.info("    # Once we have enough data, compute PCA")
    logger.info("    if len(basket_returns) >= 10 and all(len(v) > 100 for v in basket_returns.values()):")
    logger.info("        # Create returns matrix")
    logger.info("        returns_df = pd.DataFrame(basket_returns)")
    logger.info("        ")
    logger.info("        # Compute PCA (common factors)")
    logger.info("        from sklearn.decomposition import PCA")
    logger.info("        pca = PCA(n_components=3)")
    logger.info("        factors = pca.fit_transform(returns_df.dropna())")
    logger.info("        ")
    logger.info("        # Compute residuals (idiosyncratic component)")
    logger.info("        residuals = returns_df - pca.inverse_transform(factors)")
    logger.info("        ")
    logger.info("        # Trade on residual z-scores")
    logger.info("        residual_z = (residuals - residuals.mean()) / residuals.std()")
    logger.info("        ")
    logger.info("        for symbol in residual_z.columns:")
    logger.info("            z = residual_z[symbol].iloc[-1]")
    logger.info("            if z < -2.0:")
    logger.info("                logger.info(f'LONG {symbol}: residual z={z:.2f}')")
    logger.info("            elif z > 2.0:")
    logger.info("                logger.info(f'SHORT {symbol}: residual z={z:.2f}')")
    logger.info("```\n")
    
    logger.info("üìä Expected performance:")
    logger.info("   - Sharpe: 2-4 (basket stat arb)")
    logger.info("   - Correlation: Low to market")
    logger.info("   - Max DD: 5-15%")


# ============================================================================
# Main
# ============================================================================

async def main():
    """Run all strategy demonstrations."""
    logger.info("\n" + "="*70)
    logger.info("üöÄ ADVANCED CRYPTO STRATEGIES")
    logger.info("="*70)
    logger.info("\nDemonstrating sophisticated strategies using:")
    logger.info("  - Sabot streaming pipelines")
    logger.info("  - Fintech kernels (82+ operators)")
    logger.info("  - ASOF joins (time-series alignment)")
    logger.info("  - State backends (larger-than-memory)")
    
    await cross_exchange_arbitrage()
    await btc_eth_pairs_trading()
    await funding_rate_arbitrage()
    await microstructure_market_making()
    await statistical_arbitrage_basket()
    
    logger.info("\n" + "="*70)
    logger.info("‚úÖ STRATEGY DEMONSTRATIONS COMPLETE")
    logger.info("="*70)
    
    logger.info("\nüìö Key patterns:")
    logger.info("  1. ASOF joins for multi-source alignment")
    logger.info("  2. Kalman filter for time-varying relationships")
    logger.info("  3. Microstructure signals for short-term prediction")
    logger.info("  4. PCA for basket factor analysis")
    logger.info("  5. State backends for scalability")
    
    logger.info("\nüí° Next steps:")
    logger.info("  1. Implement data collectors for multiple exchanges")
    logger.info("  2. Build execution layer (REST/WebSocket APIs)")
    logger.info("  3. Add risk management and position sizing")
    logger.info("  4. Deploy to production cluster")
    
    logger.info("\nüìñ See also:")
    logger.info("  - examples/crypto_research_platform.py (main platform)")
    logger.info("  - examples/CRYPTO_RESEARCH_SETUP.md (complete guide)")
    logger.info("  - sabot/fintech/ (all kernel documentation)")


if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è  Interrupted by user")

