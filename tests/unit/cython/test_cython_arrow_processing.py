#!/usr/bin/env python3
"""
Test Cython Arrow Processing

Tests the complete Arrow processing pipeline including:
- Zero-copy batch processing
- SIMD-accelerated compute operations
- Arrow-native windowing
- High-performance joins
- Distributed data transfer via Flight
"""

import asyncio
import tempfile
import os
import sys
import time

# Add sabot to path
sys.path.insert(0, os.path.dirname(__file__))

try:
    from sabot._cython.arrow.batch_processor import ArrowBatchProcessor, ArrowComputeEngine
    from sabot._cython.arrow.window_processor import ArrowWindowProcessor
    from sabot._cython.arrow.join_processor import ArrowJoinProcessor
    from sabot._cython.arrow.flight_client import ArrowFlightClient
    ARROW_AVAILABLE = True
except ImportError:
    ARROW_AVAILABLE = False


async def test_arrow_batch_processor():
    """Test zero-copy Arrow batch processing."""
    print("üß™ Testing Arrow Batch Processor...")

    if not ARROW_AVAILABLE:
        print("‚ö†Ô∏è  Arrow not available, skipping batch processor tests")
        return

    try:
        import pyarrow as pa
        import numpy as np

        # Create test batch
        data = {
            'timestamp': [1000, 2000, 3000, 4000],
            'user_id': [1, 2, 3, 4],
            'value': [10.5, 20.3, 30.7, 40.1]
        }
        batch = pa.RecordBatch.from_pandas(pa.DataFrame(data))

        # Initialize processor
        processor = ArrowBatchProcessor()
        processor.initialize_batch(batch)

        # Test batch info
        info = processor.get_batch_info()
        assert info['initialized'] == True
        assert info['num_rows'] == 4
        assert info['num_columns'] == 3

        # Test direct column access (zero-copy)
        ts0 = processor.get_int64('timestamp', 0)
        assert ts0 == 1000

        val2 = processor.get_float64('value', 2)
        assert abs(val2 - 30.7) < 0.001

        # Test column statistics
        stats = processor.get_column_stats('timestamp')
        assert stats['length'] == 4
        assert stats['null_count'] == 0

        print("‚úÖ Arrow Batch Processor tests passed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping detailed batch processor tests")


async def test_arrow_compute_engine():
    """Test SIMD-accelerated compute operations."""
    print("üß™ Testing Arrow Compute Engine...")

    if not ARROW_AVAILABLE:
        print("‚ö†Ô∏è  Arrow not available, skipping compute engine tests")
        return

    try:
        import pyarrow as pa

        # Create test batch
        data = {
            'value': [10, 20, 30, 40, 50],
            'category': ['A', 'B', 'A', 'B', 'A']
        }
        batch = pa.RecordBatch.from_pandas(pa.DataFrame(data))

        # Test filtering
        filtered = ArrowComputeEngine.filter_batch(batch, "value > 25")
        assert filtered.num_rows == 3  # values 30, 40, 50

        # Test aggregation
        agg_result = ArrowComputeEngine.aggregate_batch(
            batch,
            "category",
            {'sum_value': ('value', 'sum'), 'count': ('value', 'count')}
        )

        # Should have groups for A and B
        assert agg_result.num_rows == 2

        # Test join
        left_data = {'id': [1, 2, 3], 'left_val': [10, 20, 30]}
        right_data = {'id': [1, 2, 4], 'right_val': [100, 200, 400]}

        left_batch = pa.RecordBatch.from_pandas(pa.DataFrame(left_data))
        right_batch = pa.RecordBatch.from_pandas(pa.DataFrame(right_data))

        joined = ArrowComputeEngine.join_batches(left_batch, right_batch, 'id', 'id', 'inner')
        assert joined.num_rows == 2  # IDs 1 and 2 match

        print("‚úÖ Arrow Compute Engine tests passed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping compute engine tests")


async def test_arrow_window_processor():
    """Test Arrow-native windowing."""
    print("üß™ Testing Arrow Window Processor...")

    if not ARROW_AVAILABLE:
        print("‚ö†Ô∏è  Arrow not available, skipping window processor tests")
        return

    try:
        import pyarrow as pa

        # Create test batch with timestamps
        timestamps = [1000, 1500, 2000, 2500, 3000, 3500]  # Every 500ms
        data = {
            'timestamp': timestamps,
            'value': [10, 20, 30, 40, 50, 60]
        }
        batch = pa.RecordBatch.from_pandas(pa.DataFrame(data))

        processor = ArrowWindowProcessor('timestamp', 'window_id')

        # Test tumbling windows (2-second windows)
        windowed = processor.assign_tumbling_windows(batch, 2000)
        assert 'window_id' in windowed.schema.names

        # Check window assignments
        window_ids = windowed['window_id'].to_pylist()
        expected_windows = [0, 0, 2000, 2000, 2000, 2000]  # Based on timestamp // 2000 * 2000
        assert window_ids == expected_windows

        # Test sliding windows (2-second windows, 1-second slide)
        sliding = processor.assign_sliding_windows(batch, 2000, 1000)
        # This will expand the batch since records belong to multiple windows

        # Test aggregation on windows
        agg_result = processor.aggregate_windows(
            windowed,
            "",
            {'sum_value': ('value', 'sum'), 'count': ('value', 'count')}
        )
        assert agg_result.num_rows > 0

        print("‚úÖ Arrow Window Processor tests passed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping window processor tests")


async def test_arrow_join_processor():
    """Test Flink-style joins."""
    print("üß™ Testing Arrow Join Processor...")

    if not ARROW_AVAILABLE:
        print("‚ö†Ô∏è  Arrow not available, skipping join processor tests")
        return

    try:
        import pyarrow as pa

        processor = ArrowJoinProcessor()

        # Create test batches
        left_data = {
            'id': [1, 2, 3],
            'timestamp': [1000, 2000, 3000],
            'left_val': [10, 20, 30]
        }
        right_data = {
            'id': [1, 2, 4],
            'timestamp': [1100, 2100, 4100],
            'right_val': [100, 200, 400]
        }

        left_batch = pa.RecordBatch.from_pandas(pa.DataFrame(left_data))
        right_batch = pa.RecordBatch.from_pandas(pa.DataFrame(right_data))

        # Test table-table join
        joined = processor.table_table_join(left_batch, right_batch, 'id', 'id', 'inner')
        assert joined.num_rows == 2  # IDs 1 and 2 match

        # Test interval join (1-second interval)
        interval_joined = processor.interval_join(
            left_batch, right_batch, 'id', 'id',
            'timestamp', 'timestamp', 1000, 'inner'
        )
        # Should match records within 1-second interval

        # Test temporal join
        temporal_joined = processor.temporal_join(
            left_batch, right_batch, 'id', 'id', 'timestamp'
        )
        # For each left record, find latest right record with same ID and earlier timestamp

        print("‚úÖ Arrow Join Processor tests passed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping join processor tests")


async def test_arrow_flight_client():
    """Test distributed data transfer."""
    print("üß™ Testing Arrow Flight Client...")

    # Note: Flight tests require a running Flight server
    # For now, we'll test the mock client

    try:
        import pyarrow as pa

        # Create test batch
        data = {'id': [1, 2, 3], 'value': [10, 20, 30]}
        batch = pa.RecordBatch.from_pandas(pa.DataFrame(data))

        # Test mock client (when Flight server not available)
        client = ArrowFlightClient("localhost", 8815)

        # Test connection (will use mock)
        try:
            client.connect()
        except:
            pass  # Expected if no server running

        # Test basic operations with mock
        if not client.is_connected():
            # Use mock operations
            ticket = client.put_batch("test_path", batch, {"test": "metadata"})
            retrieved_batch, metadata = client.get_batch(ticket)

            assert retrieved_batch is not None
            assert metadata == {"test": "metadata"}

            # Test flight listing
            flights = client.list_flights("*")
            assert len(flights) >= 0

        print("‚úÖ Arrow Flight Client tests passed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping flight client tests")


async def test_arrow_processing_pipeline():
    """Test complete Arrow processing pipeline."""
    print("üß™ Testing Complete Arrow Processing Pipeline...")

    if not ARROW_AVAILABLE:
        print("‚ö†Ô∏è  Arrow not available, skipping pipeline tests")
        return

    try:
        import pyarrow as pa
        import numpy as np

        # Create streaming data
        timestamps = np.arange(1000, 6000, 500)  # 10 records, 500ms apart
        data = {
            'timestamp': timestamps,
            'user_id': np.random.randint(1, 5, len(timestamps)),
            'value': np.random.randn(len(timestamps)) * 100
        }
        stream_batch = pa.RecordBatch.from_pandas(pa.DataFrame(data))

        # Step 1: Batch processing (zero-copy access)
        batch_processor = ArrowBatchProcessor()
        batch_processor.initialize_batch(stream_batch)

        # Extract timestamps for watermark processing
        start_time = time.time()
        for i in range(stream_batch.num_rows):
            ts = batch_processor.get_int64('timestamp', i)
            # In real processing, this would update watermarks
        batch_time = time.time() - start_time

        # Step 2: Windowing (tumbling windows)
        window_processor = ArrowWindowProcessor('timestamp', 'window_id')
        windowed_batch = window_processor.assign_tumbling_windows(stream_batch, 2000)

        # Step 3: Aggregation within windows
        agg_batch = window_processor.aggregate_windows(
            windowed_batch,
            "window_id",
            {
                'sum_value': ('value', 'sum'),
                'count': ('value', 'count'),
                'avg_value': ('value', 'mean')
            }
        )

        # Step 4: Joining with static data (simulate user table)
        user_data = {
            'user_id': [1, 2, 3, 4],
            'user_name': ['Alice', 'Bob', 'Charlie', 'Diana'],
            'region': ['US', 'EU', 'US', 'ASIA']
        }
        user_table = pa.RecordBatch.from_pandas(pa.DataFrame(user_data))

        join_processor = ArrowJoinProcessor()
        enriched_batch = join_processor.stream_table_join(
            windowed_batch, user_table, 'user_id', 'user_id', 'inner'
        )

        # Verify pipeline worked
        assert windowed_batch.num_rows == stream_batch.num_rows  # Tumbling windows don't expand
        assert agg_batch.num_rows > 0  # Should have aggregated results
        assert 'window_id' in windowed_batch.schema.names
        assert enriched_batch.num_rows == windowed_batch.num_rows  # Inner join

        print(f"   Pipeline processed {stream_batch.num_rows} records in {batch_time:.4f}s")
        print("‚úÖ Arrow Processing Pipeline tests passed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping pipeline tests")


async def test_performance_characteristics():
    """Test performance characteristics of Arrow processing."""
    print("üß™ Testing Arrow Processing Performance...")

    if not ARROW_AVAILABLE:
        print("‚ö†Ô∏è  Arrow not available, skipping performance tests")
        return

    try:
        import pyarrow as pa
        import numpy as np

        # Create larger test batch
        num_records = 10000
        data = {
            'timestamp': np.arange(1000000, 1000000 + num_records),
            'user_id': np.random.randint(1, 1000, num_records),
            'value': np.random.randn(num_records) * 100
        }
        batch = pa.RecordBatch.from_pandas(pa.DataFrame(data))

        # Test batch processing speed
        processor = ArrowBatchProcessor()
        processor.initialize_batch(batch)

        # Time column access
        start_time = time.time()
        total = 0
        for i in range(min(1000, num_records)):  # Test first 1000 records
            total += processor.get_int64('timestamp', i)
        access_time = time.time() - start_time

        print(f"   Column access: {access_time:.2f}s for 1000 records")
        print(f"   Access rate: {1000/access_time:.0f} records/sec")
        # Test compute performance
        compute_start = time.time()
        filtered = ArrowComputeEngine.filter_batch(batch, "value > 0")
        compute_time = time.time() - compute_start

        print(f"   Compute filter: {compute_time:.2f}s for {num_records} records")
        print(f"   Filter rate: {num_records/compute_time:.0f} records/sec")
        # Test windowing performance
        window_start = time.time()
        window_processor = ArrowWindowProcessor('timestamp', 'window_id')
        windowed = window_processor.assign_tumbling_windows(batch, 10000)  # 10-second windows
        window_time = time.time() - window_start

        print(f"   Windowing: {window_time:.4f}s for {num_records} records")
        print("‚úÖ Arrow Processing Performance tests completed!")

    except ImportError:
        print("‚ö†Ô∏è  PyArrow not available, skipping performance tests")


async def main():
    """Run all Arrow processing tests."""
    print("üöÄ Cython Arrow Processing Tests")
    print("=" * 60)

    try:
        await test_arrow_batch_processor()
        await test_arrow_compute_engine()
        await test_arrow_window_processor()
        await test_arrow_join_processor()
        await test_arrow_flight_client()
        await test_arrow_processing_pipeline()
        await test_performance_characteristics()

        print("\nüéâ All Arrow processing tests passed!")
        print("‚úÖ Zero-copy Arrow processing is working")
        print("‚úÖ SIMD-accelerated operations are functional")
        print("‚úÖ Flink-compatible joins and windowing implemented")

    except Exception as e:
        print(f"\n‚ùå Tests failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
