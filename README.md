# Sabot: High-Performance Python Data Processing

**‚ö†Ô∏è EXPERIMENTAL - ALPHA SOFTWARE ‚ö†Ô∏è**

**The Python alternative to PySpark and Ray for high-performance columnar data processing.**

Sabot is a Python framework that brings Apache Arrow's columnar performance to data processing workflows. Unlike PySpark's JVM overhead or Ray's distributed complexity, Sabot provides zero-copy Arrow operations with Cython acceleration for massive throughput on single machines.

```python
import sabot as sb

# Create app with Kafka
app = sb.App('fraud-detection', broker='kafka://localhost:19092')

# Define streaming agent
@app.agent('transactions')
async def detect_fraud(stream):
    async for transaction in stream:
        if is_fraudulent(transaction):
            yield alert

# Deploy with CLI
# $ sabot -A myapp:app worker
```

## Project Status

This is an experimental research project exploring the design space of:
- Zero-copy Arrow columnar processing in Python
- Cython acceleration for data-intensive operations
- High-performance batch processing alternatives
- Kafka streaming integration with columnar efficiency

**Current State (v0.1.0-alpha):**
- ‚úÖ **CyArrow**: Production-ready zero-copy Arrow operations (104M rows/sec joins)
- ‚úÖ **DataLoader**: High-performance CSV/Arrow IPC loading (52x faster than CSV)
- ‚úÖ **Streaming Agents**: Faust-inspired Kafka processing with columnar data
- ‚úÖ **Cython Acceleration**: SIMD-accelerated compute kernels
- ‚ö†Ô∏è Distributed features are experimental (checkpoints, state management)
- ‚ö†Ô∏è Test coverage is limited (~5%)
- ‚ö†Ô∏è Not recommended for production use

## Performance: Why Choose Sabot Over PySpark/Ray?

**Sabot delivers PySpark-level performance without the JVM overhead:**

### Core Data Processing (M3 MacBook Pro, 11.2M rows)
- **Hash Joins**: 104M rows/sec (11.2M row join in 107ms)
- **Data Loading (Arrow IPC)**: 5M rows/sec (10M rows in 2 seconds, memory-mapped)
- **Data Loading (CSV)**: 0.5-1.0M rows/sec (multi-threaded)
- **Arrow IPC vs CSV**: 52x faster loading (10M rows: 2s vs 103s)
- **File Compression**: 50-70% size reduction (5.6GB ‚Üí 2.4GB)
- **Zero-copy operations**: ~2-3ns per element (SIMD-accelerated)

**Fintech Enrichment Pipeline (Complete workflow):**
- **Total Pipeline**: 2.3 seconds end-to-end
- **Hash Join**: 104.6M rows/sec throughput
- **Window Operations**: ~2-3ns per element (SIMD-accelerated)
- **Spread Calculation**: Vectorized compute kernels
- **Data Loading**: 2.1 seconds (10M + 1.2M rows)

**vs PySpark (same workload, anecdotal):**
- PySpark: ~30-60 seconds (JVM startup + serialization overhead)
- Sabot: 2.3 seconds (pure Python + Arrow + Cython)

**vs Ray:**
- Ray: Distributed coordination overhead even for single-machine
- Sabot: Direct columnar operations with zero serialization

### Streaming Performance
**Real-time Fraud Detection (Kafka + columnar processing):**
- **Throughput**: 143K-260K transactions/sec
- **Latency p50/p95/p99**: 0.01ms / 0.01ms / 0.01ms
- **Pattern Detection**: velocity, amount anomaly, geo-impossible
- **Stateful Processing**: 1M+ state operations/sec

**Experimental Features (In Development):**
- Distributed agent runtime (Ray-like actor model)
- RocksDB state backend (persistent state)
- GPU acceleration via RAFT
- Complex event processing (CEP)

### Auto-Numba UDF Compilation

**NEW: Automatic 10-100x speedup for user-defined functions**

Sabot automatically compiles Python UDFs with Numba JIT for massive performance gains. Works seamlessly with batch-first processing:

```python
# User writes normal Python - Sabot auto-compiles it!
def my_transform(batch):
    # Extract numpy arrays from Arrow columns
    values = batch.column('value').to_numpy()
    results = []

    # Numba-compiled computation
    for i in range(len(values)):
        total = 0
        for j in range(100):
            total += values[i] * j
        results.append(total)

    # Return new RecordBatch
    return batch.append_column('computed', pa.array(results))

# Automatically compiled with Numba @njit (10-50x faster)
stream = Stream.from_kafka('data').map(my_transform)
```

**How it works:**
1. AST analysis detects function patterns (loops, NumPy array ops, etc.)
2. Chooses optimal compilation strategy (`@njit` vs `@vectorize`)
3. Compiles with Numba transparently for batch processing
4. Falls back to Python if compilation fails
5. Caches compiled functions for reuse

**Performance:**
- Scalar loops: 10-50x speedup
- NumPy operations: 50-100x speedup
- Compilation overhead: <100ms (first-time only)
- Cache hit: <1ms

**Works with batch-first architecture!**

## Design Goals

üöÄ **PySpark Performance in Pure Python**
- **CyArrow**: Zero-copy Arrow operations (104M rows/sec joins)
- **Arrow IPC**: Memory-mapped data loading (52x faster than CSV)
- **SIMD Acceleration**: Vectorized operations beating PySpark throughput
- **Cython DataLoader**: Multi-threaded CSV parsing, auto-format detection

‚ö° **Ray-Like Distributed Processing (Experimental)**
- Actor-based agents for distributed computation
- Distributed checkpointing (Chandy-Lamport barriers)
- Complex event processing (CEP) with pattern matching
- Stateful stream processing with persistence backends

üîß **Pythonic API - No JVM, No Serialization**
- Unified imports: `import sabot as sb`
- Zero-copy operations: `from sabot.cyarrow import load_data, hash_join_batches`
- Decorator-based agents: `@app.agent()` (experimental - see note below)
- Multiple data formats: Arrow IPC, CSV, Parquet, Avro

**Note on Agent API:** The `@app.agent()` decorator is experimental. Agents register successfully but stream consumption requires manual message deserialization. See `examples/fraud_app.py` for working pattern.

## Quick Start

### 1. Install

**Prerequisites:** Python 3.9+, C++ compiler, CMake 3.16+

```bash
# Clone with vendored Arrow C++ submodule
git clone --recursive https://github.com/yourusername/sabot.git
cd sabot

# Option A: Quick install (builds Arrow C++ automatically, ~30-60 mins first time)
pip install -e .

# Option B: Build Arrow C++ manually first (recommended for development)
python build.py          # One-time Arrow build (~30-60 mins)
pip install -e .         # Fast install (<1 min)

# Verify vendored Arrow is working
python -c "from sabot import cyarrow; print(f'Vendored Arrow: {cyarrow.USING_ZERO_COPY}')"

# Note: Use sabot.cyarrow (our optimized Arrow), not pyarrow

# Dependencies Note: We vendor critical Cython dependencies (RocksDB, Arrow components)
# because they are tightly bound to native C/C++ libraries, not standard Python packages.
# This ensures consistent builds and performance across platforms.
```

### 2. Start Infrastructure

```bash
# Start Kafka, Postgres, Redis via Docker
docker compose up -d

# Check services
docker compose ps
```

### 3. Create Your First App

**`fraud_app.py`:**
```python
import sabot as sb
import json

# Create Sabot application
app = sb.App(
    'fraud-detection',
    broker='kafka://localhost:19092',
    value_serializer='json'
)

# Define fraud detector with state
detector = FraudDetector()  # Your fraud detection logic

@app.agent('bank-transactions')
async def detect_fraud(stream):
    """Process transactions and detect fraud patterns."""
    async for message in stream:
        # Deserialize message (stream yields raw bytes/str)
        if isinstance(message, bytes):
            txn = json.loads(message.decode('utf-8'))
        else:
            txn = json.loads(message) if isinstance(message, str) else message

        # Process transaction
        alerts = await detector.detect_fraud(txn)

        # Handle fraud alerts
        for alert in alerts:
            print(f"üö® FRAUD: {alert['type']} - {alert['details']}")
```

### 4. Run with CLI

```bash
# Start worker (Faust-style)
sabot -A fraud_app:app worker --loglevel=INFO

# Or with concurrency
sabot -A fraud_app:app worker -c 4
```

## Architecture

Sabot combines **Arrow's columnar performance** with **Python's ecosystem**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Application Layer                    ‚îÇ
‚îÇ   @app.agent() decorators, Faust-style API            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Sabot Core (Clean API)                     ‚îÇ
‚îÇ   import sabot as sb                                    ‚îÇ
‚îÇ   - sb.App, sb.agent()                                  ‚îÇ
‚îÇ   - sb.Barrier, sb.BarrierTracker (checkpoints)        ‚îÇ
‚îÇ   - sb.MemoryBackend, sb.ValueState (state)            ‚îÇ
‚îÇ   - sb.WatermarkTracker, sb.Timers (time)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Cython-Accelerated Modules (10-100x faster)     ‚îÇ
‚îÇ   - Checkpoint coordination (Chandy-Lamport)            ‚îÇ
‚îÇ   - State management (RocksDB, memory)                  ‚îÇ
‚îÇ   - Time/watermark tracking                             ‚îÇ
‚îÇ   - Arrow batch processing (SIMD-accelerated)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Infrastructure Layer                   ‚îÇ
‚îÇ   Kafka, Redpanda | PostgreSQL | Redis | RocksDB       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Modules

| Module | Description | Performance |
|--------|-------------|-------------|
| **cyarrow** | Zero-copy Arrow operations (hash joins, windows) | 104M rows/sec |
| **DataLoader** | Multi-threaded data loading (CSV, Arrow IPC) | 5M rows/sec (Arrow) |
| **checkpoint** | Distributed snapshots (Chandy-Lamport) | <10Œºs initiation |
| **state** | Managed state (Memory, RocksDB, Redis) | 1M+ ops/sec |
| **time** | Watermarks, timers, event-time | <5Œºs tracking |
| **agents** | Actor-based stream processors (experimental) | - |

## Example: Fintech Data Enrichment (Zero-Copy Arrow)

The fintech enrichment demo shows how Sabot processes **millions of rows with Arrow columnar operations**:

```bash
# One-time: Convert CSV to Arrow IPC format (52x faster loading)
cd examples/fintech_enrichment_demo
python convert_csv_to_arrow.py

# Enable Arrow IPC and run
export SABOT_USE_ARROW=1
./run_demo.sh --securities 10000000 --quotes 1200000
```

**How Arrow Batch Processing Works:**
- **Arrow Joins**: Convert RecordBatches to Tables, perform SIMD-accelerated hash joins
- **Zero-Copy Operations**: Direct memory access to Arrow buffers (no Python object creation)
- **SIMD Acceleration**: PyArrow compute kernels use vectorized operations
- **Memory-Mapped Loading**: Arrow IPC files loaded directly into memory without copying

**Performance Results (M3 MacBook Pro, 11.2M rows):**
- **Total Pipeline**: 2.3 seconds end-to-end (vs 103s with CSV)
- **Hash Join**: 104.6M rows/sec (11.2M rows joined in 107ms)
- **Data Loading**: 2.1 seconds (10M securities + 1.2M quotes)
- **File Size**: 50-70% smaller than CSV (5.6GB ‚Üí 2.4GB)
- **Speedup**: 46x faster than traditional CSV processing

**vs PySpark for this workload:**
- PySpark: ~30-60 seconds (JVM startup, serialization, garbage collection)
- Sabot: 2.3 seconds (pure Python + Arrow + Cython acceleration)

See [DATA_FORMATS.md](DATA_FORMATS.md) for format guide and [PERFORMANCE_SUMMARY.md](PERFORMANCE_SUMMARY.md) for detailed benchmarks.

## CLI Reference

Sabot provides a Faust-style CLI for production deployments:

```bash
# Start worker
sabot -A myapp:app worker

# With concurrency
sabot -A myapp:app worker -c 4

# Override broker
sabot -A myapp:app worker -b kafka://prod:9092

# Set log level
sabot -A myapp:app worker --loglevel=DEBUG

# Full options
sabot -A myapp:app worker \
  --concurrency 4 \
  --broker kafka://localhost:9092 \
  --loglevel INFO
```

## API Reference

### Creating an App

```python
import sabot as sb

app = sb.App(
    'my-app',
    broker='kafka://localhost:19092',      # Kafka broker URL
    value_serializer='json',                # 'json', 'arrow', 'avro'
    enable_distributed_state=True,          # Use Redis for state
    database_url='postgresql://localhost/sabot'  # For durable execution
)
```

### Defining Agents

```python
@app.agent('my-topic')
async def process_events(stream):
    """Stateful event processor."""
    async for event in stream:
        # Process event
        result = transform(event)
        # Yield to output
        yield result
```

### State Management

```python
# Memory backend (fast, not persistent)
config = sb.BackendConfig(
    backend_type="memory",
    max_size=100000,
    ttl_seconds=300.0
)
backend = sb.MemoryBackend(config)

# RocksDB backend (persistent, larger state)
rocksdb = sb.RocksDBBackend(
    sb.BackendConfig(backend_type="rocksdb", path="./state")
)

# State types
value_state = sb.ValueState(backend, "counter")      # Single value
map_state = sb.MapState(backend, "user_profiles")    # Key-value map
list_state = sb.ListState(backend, "events")         # Ordered list
```

### Checkpointing

```python
# Barrier tracking for distributed checkpoints
tracker = sb.BarrierTracker(num_channels=3)

# Register barrier
aligned = tracker.register_barrier(
    channel=0,
    checkpoint_id=1,
    total_inputs=3
)

# Checkpoint coordinator
coordinator = sb.Coordinator()
```

### Time & Watermarks

```python
# Track watermarks across partitions
watermark_tracker = sb.WatermarkTracker(num_partitions=3)
watermark_tracker.update_watermark(partition_id=0, timestamp=12345)

# Timer service for delayed processing
timers = sb.Timers()
```

## Installation Details

### System Requirements

- **Python**: 3.8+
- **OS**: Linux, macOS (Windows via WSL)
- **Memory**: 4GB+ recommended
- **Dependencies**: See `requirements.txt`

### Optional Dependencies

```bash
# GPU acceleration (RAFT)
pip install cudf cupy raft-dask pylibraft

# Kafka support
pip install confluent-kafka aiokafka

# Redis state backend
pip install redis hiredis

# RocksDB state backend
pip install rocksdb

# All optional features
pip install sabot[all]
```

### Building from Source

```bash
# Install Cython and dependencies
uv pip install cython numpy  # Use sabot.cyarrow, not pyarrow

# Build Cython extensions
python setup.py build_ext --inplace

# Install in development mode
pip install -e .
```

## Docker Compose Infrastructure

The included `docker-compose.yml` provides a complete streaming stack:

```yaml
services:
  redpanda:      # Kafka-compatible broker (port 19092)
  console:       # Redpanda web UI (port 8080)
  postgres:      # PostgreSQL for durable execution (port 5432)
  redis:         # Redis for distributed state (port 6379)
```

**Start all services:**
```bash
docker compose up -d
```

**Access Redpanda Console:**
```bash
open http://localhost:8080
```

**View logs:**
```bash
docker compose logs -f redpanda
```

**Stop all services:**
```bash
docker compose down
```

## Examples

| Example | Description | Performance | Location |
|---------|-------------|-------------|----------|
| **Fintech Enrichment** | 11.2M row joins with Arrow IPC | 104M rows/sec | `examples/fintech_enrichment_demo/` |
| **Arrow Data Loading** | CSV to Arrow IPC conversion | 52x faster | `examples/fintech_enrichment_demo/convert_csv_to_arrow.py` |
| **Zero-Copy Operations** | Hash joins, windows, filtering | SIMD-accelerated | `examples/fintech_enrichment_demo/arrow_optimized_enrichment.py` |
| **Fraud Detection** | Real-time fraud detection (experimental) | 3-6K txn/s | `examples/fraud_app.py` |

## Benchmark Results

**Fintech Enrichment Demo (M3 MacBook Pro, 11.2M rows):**
- **Hash Join**: 104.6M rows/sec (11.2M rows in 107ms)
- **Arrow IPC Loading**: 5M rows/sec (10M rows in 2 seconds)
- **CSV Loading**: 0.5M rows/sec (multi-threaded)
- **Total Pipeline**: 2.3 seconds (vs 103s with CSV - **46x faster**)
- **File Size**: 50-70% reduction (5.6GB ‚Üí 2.4GB)
- **Memory Usage**: Memory-mapped (minimal footprint)

**CyArrow Zero-Copy Operations:**
- **Window Computation**: ~2-3ns per element (SIMD)
- **Filtering**: 50-100x faster than Python loops
- **Sorting**: 10M+ rows/sec with zero-copy slicing
- **Buffer Access**: ~5ns per element (direct C++ pointers)

**State Backend Operations (MemoryBackend):**
- **Get/Put latency**: Sub-millisecond
- **Sustained throughput**: 1M+ operations/second (Cython)

**Notes on Benchmarks:**
- Measured on M3 MacBook Pro (8-core, 18GB RAM)
- Arrow IPC with memory-mapped I/O
- Real fintech data (10M securities, 1.2M quotes)
- All benchmarks are reproducible (see `PERFORMANCE_SUMMARY.md`)

## Documentation

### Performance & Data Formats
- **[PERFORMANCE_SUMMARY.md](PERFORMANCE_SUMMARY.md)** - Benchmark results and analysis
- **[DATA_FORMATS.md](DATA_FORMATS.md)** - Format comparison (Arrow IPC, CSV, Parquet)
- **[CYARROW.md](CYARROW.md)** - CyArrow API reference and zero-copy operations
- **[DEMO_QUICKSTART.md](DEMO_QUICKSTART.md)** - Fintech demo quick start

### Architecture & Development
- **[PROJECT_MAP.md](PROJECT_MAP.md)** - Directory structure and module overview
- **[Architecture](docs/ARCHITECTURE.md)** - Deep dive into internals (if exists)
- **[API Reference](docs/API_REFERENCE.md)** - API documentation (if exists)

## Comparison to Other Frameworks

| Feature | Sabot | PySpark | Ray | Apache Flink |
|---------|-------|---------|-----|--------------|
| **Language** | Python | Python (JVM) | Python | Java/Scala |
| **Performance** | ‚úÖ **104M rows/sec** (Arrow + Cython) | üêå 10-50x slower (JVM serialization) | ‚ö†Ô∏è Distributed overhead | ‚úÖ Production-scale |
| **Columnar Processing** | ‚úÖ **Zero-copy Arrow** (SIMD-accelerated) | ‚ö†Ô∏è Arrow integration | ‚ùå No | ‚ö†Ô∏è Limited |
| **Data Loading** | ‚úÖ **52x faster** (Arrow IPC) | üêå Standard | üêå Standard | üêå Standard |
| **Memory Efficiency** | ‚úÖ **Memory-mapped** (no copies) | üêå JVM heap | ‚ö†Ô∏è Object serialization | ‚úÖ Native |
| **Setup Complexity** | ‚úÖ **Single pip install** | üêå JVM + Spark cluster | üêå Distributed setup | üêå Cluster management |
| **Debugging** | ‚úÖ **Pure Python** (pdb, breakpoints) | üêå JVM stack traces | ‚ö†Ô∏è Distributed complexity | üêå JVM debugging |
| **Streaming** | ‚ö†Ô∏è Experimental agents | ‚úÖ Structured Streaming | ‚úÖ Ray Data | ‚úÖ Production |
| **Production Ready** | ‚úÖ CyArrow (yes), ‚ö†Ô∏è Streaming (no) | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |

## Roadmap

### Current Status (v0.1.0-alpha)
**Working (Production-Quality):**
- ‚úÖ **CyArrow**: Zero-copy hash joins (104M rows/sec)
- ‚úÖ **DataLoader**: Multi-threaded CSV, memory-mapped Arrow IPC
- ‚úÖ **Arrow IPC Format**: 52x faster than CSV, 50-70% smaller files
- ‚úÖ **SIMD Operations**: Window functions, filtering, sorting
- ‚úÖ **Cython checkpoint coordinator** (Chandy-Lamport barriers)
- ‚úÖ **Memory state backend** with Cython acceleration
- ‚úÖ **Fintech enrichment demo** (11.2M rows in 2.3s)

**Working (Experimental):**
- ‚ö†Ô∏è Basic Kafka source/sink with schema registry
- ‚ö†Ô∏è Watermark tracking primitives
- ‚ö†Ô∏è CLI scaffolding (Faust-style)
- ‚ö†Ô∏è Fraud detection demo (3K-6K txn/s)

**In Progress:**
- üöß Agent runtime execution layer (partially stubbed)
- üöß RocksDB state backend integration
- üöß Distributed coordination
- üöß Complex event processing (CEP)

**Known Limitations:**
- ‚ö†Ô∏è Test coverage ~5% for streaming features
- ‚ö†Ô∏è Agent runtime has mock implementations
- ‚ö†Ô∏è CLI uses stubs in places
- ‚úÖ CyArrow & DataLoader are well-tested and performant

### Planned (v0.2.0)
- üéØ Complete agent runtime implementation
- üéØ Comprehensive integration tests
- üéØ RocksDB state backend completion
- üéØ Improved error handling and recovery
- üéØ Performance benchmarking suite
- üéØ Production-ready checkpointing

### Future Ideas (v0.3.0+)
- üìã GPU acceleration via RAFT
- üìã Advanced CEP patterns
- üìã SQL/Table API
- üìã Web UI for monitoring
- üìã S3/HDFS connectors
- üìã Query optimizer

## Contributing

Sabot is an experimental research project and welcomes contributions! This is a learning-focused project exploring streaming architecture design.

**High-Impact Areas:**
1. **Testing**: Expand test coverage beyond current ~5%
2. **Agent Runtime**: Complete the execution layer implementation
3. **RocksDB Integration**: Finish state backend implementation
4. **Documentation**: Improve guides and examples
5. **Benchmarking**: Add comprehensive performance tests

**Before Contributing:**
- This is alpha software with many incomplete features
- Focus on learning and experimentation rather than production readiness
- Check existing issues and roadmap before starting major work
- Add tests for any new functionality

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines (if available).

## License

Apache License 2.0 - See [LICENSE](LICENSE) file for details.

## Credits

Inspired by:
- **Apache Arrow** - Zero-copy columnar data processing
- **PySpark** - DataFrame API and distributed processing concepts
- **Ray** - Actor model and distributed computing patterns
- **Faust** - Python streaming framework, CLI design, and agent patterns

Built with:
- **Cython** - High-performance compiled modules for Arrow acceleration
- **PyArrow** - SIMD-accelerated compute kernels and memory management
- **Redpanda** - Kafka-compatible streaming infrastructure
- **PostgreSQL** - Durable execution (DBOS-inspired)
- **RocksDB** - Embedded key-value store for state management

## Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/sabot/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/sabot/discussions)
- **Email**: team@sabot.io

---

## When to Choose Sabot vs PySpark vs Ray

**Choose Sabot when:**
- You need **PySpark-level performance** without JVM overhead
- You're processing **large columnar datasets** (Arrow IPC, Parquet)
- You want **pure Python debugging** (pdb, breakpoints, no JVM stack traces)
- **Single-machine performance** is your primary concern
- You need **fast iteration** during development

**Choose PySpark when:**
- You need **production-scale distributed processing**
- Your team is already invested in the Spark ecosystem
- You have **existing Spark clusters** and infrastructure
- **Java/Scala performance** is acceptable for your use case

**Choose Ray when:**
- You need **distributed actor-based processing**
- You're building **complex ML pipelines** with distributed training
- **Python-first distributed computing** is your priority

**This is experimental alpha software.** The CyArrow columnar processing is production-quality, but streaming features are experimental. We welcome feedback and contributions!

**Ready to try it?** Check out the [Fintech Enrichment Demo](examples/fintech_enrichment_demo/) to see Sabot processing millions of rows in seconds!
